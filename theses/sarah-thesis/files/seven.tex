%% Copyright 1998 Pepe Kubon
%%
%% `one.tex' --- 1st chapter for thes-full.tex, thes-short-tex from
%%                the `csthesis' bundle
%%
%% You are allowed to distribute this file together with all files
%% mentioned in READ.ME.
%%
%% You are not allowed to modify its contents.
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       Chapter 1 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Summary and Conclusion}\label{chap:seven}

Outlier detection is an important task in data mining and has many applications in areas such as health care, security and finance. While many outlier analysis techniques have been developed for i.i.d. propositional data, there are not many methods designed for structured data. In this dissertation, we developed two model-based outlier detection methods for object-relational data model.
 
In Chapter \ref{chap:four} we developed a pipeline propositionalization approach where the information from multiple data tables is summarized in a single data table. We utilized Markov Logic Network learning for this task. In an empirical comparison with the baseline wordification approach of enumerating all conjunctive formulas up to length 2, Markov Logic propositionalization showed several advantages: 1) The set of formulas learned was substantially smaller, leading to smaller data tables and faster outlier detection. 2) The formulas learned were longer, representing more complex relational patterns. 3) For a fixed single-table outlier analysis method, the average detection accuracy was higher.


In Chapter \ref{chap:five} we presented a new approach for applying Bayes nets to object-relational outlier detection. The key idea is to learn one set of parameter values that represent class-level associations, another set to represent object-level associations, and compare how well each parametrization fits the relational data that characterize the target object. The classic metric for comparing two parametrized models is their log-likelihood ratio; we refined this concept to define  a new relational log-likelihood distance metric via two transformations:  (1) A mutual information decomposition, and (2) replacing log-likelihood differences by log-likelihood distances. This metric combines a single feature component, where features are treated as independent, with a correlation component that measures the deviation in the features' mutual information.


%need to check Horn clauses


In Chapter \ref{chap:six}\ we used the $\mid$ metric, that was introduced in Chapter~\ref{chap:five}, to rank the individuals. We compared the $\mid$ metric to other metrics of success for a given domain.
In our experimental results we showed that the $\mid$ metric correlates to a surprising degree with success metrics across different domains and classes of individuals. Since high success is an independent metric that indicates an unusual individual, this correlation shows that $\mid$ marks meaningful and interesting outliers.

