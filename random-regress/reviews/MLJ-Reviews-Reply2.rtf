{\rtf1\adeflang1025\ansi\ansicpg10000\uc1\adeff0\deff0\stshfdbch0\stshfloch0\stshfhich0\stshfbi0\deflang1033\deflangfe1033\themelang4105\themelangfe0\themelangcs0{\fonttbl{\f0\fbidi \fnil\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\f5\fbidi \fnil\fcharset0\fprq2{\*\panose 00000000000000000000}Helvetica;}{\f14\fbidi \fnil\fcharset2\fprq2{\*\panose 05000000000000000000}Wingdings;}{\f14\fbidi \fnil\fcharset2\fprq2{\*\panose 05000000000000000000}Wingdings;}{\flomajor\f31500\fbidi \fnil\fcharset0\fprq2{\*\panose 020b0604020202020204}Arial;}{\fdbmajor\f31501\fbidi \fnil\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\fhimajor\f31502\fbidi \fnil\fcharset0\fprq2{\*\panose 020f0502020204030204}Calibri;}{\fbimajor\f31503\fbidi \fnil\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\flominor\f31504\fbidi \fnil\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\fdbminor\f31505\fbidi \fnil\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\fhiminor\f31506\fbidi \fnil\fcharset0\fprq2{\*\panose 02040503050406030204}Cambria;}{\fbiminor\f31507\fbidi \fnil\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\f687\fbidi \fnil\fcharset238\fprq2 Times New Roman CE;}{\f688\fbidi \fnil\fcharset204\fprq2 Times New Roman Cyr;}{\f690\fbidi \fnil\fcharset161\fprq2 Times New Roman Greek;}{\f691\fbidi \fnil\fcharset162\fprq2 Times New Roman Tur;}{\f692\fbidi \fnil\fcharset177\fprq2 Times New Roman (Hebrew);}{\f693\fbidi \fnil\fcharset178\fprq2 Times New Roman (Arabid);}{\f694\fbidi \fnil\fcharset186\fprq2 Times New Roman Baltic;}{\f695\fbidi \fnil\fcharset163\fprq2 Times New Roman (Vietnamese);}{\f737\fbidi \fnil\fcharset238\fprq2 Helvetica CE;}{\f738\fbidi \fnil\fcharset204\fprq2 Helvetica Cyr;}{\f740\fbidi \fnil\fcharset161\fprq2 Helvetica Greek;}{\f741\fbidi \fnil\fcharset162\fprq2 Helvetica Tur;}{\f744\fbidi \fnil\fcharset186\fprq2 Helvetica Baltic;}{\f745\fbidi \fnil\fcharset163\fprq2 Helvetica (Vietnamese);}{\flomajor\f31508\fbidi \fnil\fcharset238\fprq2 Arial CE;}{\flomajor\f31509\fbidi \fnil\fcharset204\fprq2 Arial Cyr;}{\flomajor\f31511\fbidi \fnil\fcharset161\fprq2 Arial Greek;}{\flomajor\f31512\fbidi \fnil\fcharset162\fprq2 Arial Tur;}{\flomajor\f31513\fbidi \fnil\fcharset177\fprq2 Arial (Hebrew);}{\flomajor\f31514\fbidi \fnil\fcharset178\fprq2 Arial (Arabid);}{\flomajor\f31515\fbidi \fnil\fcharset186\fprq2 Arial Baltic;}{\flomajor\f31516\fbidi \fnil\fcharset163\fprq2 Arial (Vietnamese);}{\fdbmajor\f31518\fbidi \fnil\fcharset238\fprq2 Times New Roman CE;}{\fdbmajor\f31519\fbidi \fnil\fcharset204\fprq2 Times New Roman Cyr;}{\fdbmajor\f31521\fbidi \fnil\fcharset161\fprq2 Times New Roman Greek;}{\fdbmajor\f31522\fbidi \fnil\fcharset162\fprq2 Times New Roman Tur;}{\fdbmajor\f31523\fbidi \fnil\fcharset177\fprq2 Times New Roman (Hebrew);}{\fdbmajor\f31524\fbidi \fnil\fcharset178\fprq2 Times New Roman (Arabid);}{\fdbmajor\f31525\fbidi \fnil\fcharset186\fprq2 Times New Roman Baltic;}{\fdbmajor\f31526\fbidi \fnil\fcharset163\fprq2 Times New Roman (Vietnamese);}{\fhimajor\f31528\fbidi \fnil\fcharset238\fprq2 Calibri CE;}{\fhimajor\f31529\fbidi \fnil\fcharset204\fprq2 Calibri Cyr;}{\fhimajor\f31531\fbidi \fnil\fcharset161\fprq2 Calibri Greek;}{\fhimajor\f31532\fbidi \fnil\fcharset162\fprq2 Calibri Tur;}{\fhimajor\f31535\fbidi \fnil\fcharset186\fprq2 Calibri Baltic;}{\fhimajor\f31536\fbidi \fnil\fcharset163\fprq2 Calibri (Vietnamese);}{\fbimajor\f31538\fbidi \fnil\fcharset238\fprq2 Times New Roman CE;}{\fbimajor\f31539\fbidi \fnil\fcharset204\fprq2 Times New Roman Cyr;}{\fbimajor\f31541\fbidi \fnil\fcharset161\fprq2 Times New Roman Greek;}{\fbimajor\f31542\fbidi \fnil\fcharset162\fprq2 Times New Roman Tur;}{\fbimajor\f31543\fbidi \fnil\fcharset177\fprq2 Times New Roman (Hebrew);}{\fbimajor\f31544\fbidi \fnil\fcharset178\fprq2 Times New Roman (Arabid);}{\fbimajor\f31545\fbidi \fnil\fcharset186\fprq2 Times New Roman Baltic;}{\fbimajor\f31546\fbidi \fnil\fcharset163\fprq2 Times New Roman (Vietnamese);}{\flominor\f31548\fbidi \fnil\fcharset238\fprq2 Times New Roman CE;}{\flominor\f31549\fbidi \fnil\fcharset204\fprq2 Times New Roman Cyr;}{\flominor\f31551\fbidi \fnil\fcharset161\fprq2 Times New Roman Greek;}{\flominor\f31552\fbidi \fnil\fcharset162\fprq2 Times New Roman Tur;}{\flominor\f31553\fbidi \fnil\fcharset177\fprq2 Times New Roman (Hebrew);}{\flominor\f31554\fbidi \fnil\fcharset178\fprq2 Times New Roman (Arabid);}{\flominor\f31555\fbidi \fnil\fcharset186\fprq2 Times New Roman Baltic;}{\flominor\f31556\fbidi \fnil\fcharset163\fprq2 Times New Roman (Vietnamese);}{\fdbminor\f31558\fbidi \fnil\fcharset238\fprq2 Times New Roman CE;}{\fdbminor\f31559\fbidi \fnil\fcharset204\fprq2 Times New Roman Cyr;}{\fdbminor\f31561\fbidi \fnil\fcharset161\fprq2 Times New Roman Greek;}{\fdbminor\f31562\fbidi \fnil\fcharset162\fprq2 Times New Roman Tur;}{\fdbminor\f31563\fbidi \fnil\fcharset177\fprq2 Times New Roman (Hebrew);}{\fdbminor\f31564\fbidi \fnil\fcharset178\fprq2 Times New Roman (Arabid);}{\fdbminor\f31565\fbidi \fnil\fcharset186\fprq2 Times New Roman Baltic;}{\fdbminor\f31566\fbidi \fnil\fcharset163\fprq2 Times New Roman (Vietnamese);}{\fhiminor\f31568\fbidi \fnil\fcharset238\fprq2 Cambria CE;}{\fhiminor\f31569\fbidi \fnil\fcharset204\fprq2 Cambria Cyr;}{\fhiminor\f31571\fbidi \fnil\fcharset161\fprq2 Cambria Greek;}{\fhiminor\f31572\fbidi \fnil\fcharset162\fprq2 Cambria Tur;}{\fhiminor\f31575\fbidi \fnil\fcharset186\fprq2 Cambria Baltic;}{\fhiminor\f31576\fbidi \fnil\fcharset163\fprq2 Cambria (Vietnamese);}{\fbiminor\f31578\fbidi \fnil\fcharset238\fprq2 Times New Roman CE;}{\fbiminor\f31579\fbidi \fnil\fcharset204\fprq2 Times New Roman Cyr;}{\fbiminor\f31581\fbidi \fnil\fcharset161\fprq2 Times New Roman Greek;}{\fbiminor\f31582\fbidi \fnil\fcharset162\fprq2 Times New Roman Tur;}{\fbiminor\f31583\fbidi \fnil\fcharset177\fprq2 Times New Roman (Hebrew);}{\fbiminor\f31584\fbidi \fnil\fcharset178\fprq2 Times New Roman (Arabid);}{\fbiminor\f31585\fbidi \fnil\fcharset186\fprq2 Times New Roman Baltic;}{\fbiminor\f31586\fbidi \fnil\fcharset163\fprq2 Times New Roman (Vietnamese);}}{\colortbl;\red0\green0\blue0;\red0\green0\blue255;\red0\green255\blue255;\red0\green255\blue0;\red255\green0\blue255;\red255\green0\blue0;\red255\green255\blue0;\red255\green255\blue255;\red0\green0\blue128;\red0\green128\blue128;\red0\green128\blue0;\red128\green0\blue128;\red128\green0\blue0;\red128\green128\blue0;\red128\green128\blue128;\red192\green192\blue192;\chyperlink\ctint255\cshade255\red0\green0\blue255;}{\*\defchp }{\*\defpap \ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 }\noqfpromote {\stylesheet{\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 \rtlch\fcs1 \af0\afs20\alang4105 \ltrch\fcs0 \fs20\lang4105\langfe1033\cgrid\langnp4105\langfenp1033 \snext0 \sqformat \spriority0 Normal;}{\*\cs10 \additive \ssemihidden \sunhideused \spriority1 Default Paragraph Font;}{\*\ts11\tsrowd\trftsWidthB3\trpaddl108\trpaddr108\trpaddfl3\trpaddft3\trpaddfb3\trpaddfr3\tblind0\tblindtype3\tsvertalt\tsbrdrt\tsbrdrl\tsbrdrb\tsbrdrr\tsbrdrdgl\tsbrdrdgr\tsbrdrh\tsbrdrv \ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 \rtlch\fcs1 \af0\afs20\alang4105 \ltrch\fcs0 \fs20\lang4105\langfe1033\cgrid\langnp4105\langfenp1033 \snext11 \ssemihidden \sunhideused Normal Table;}{\*\cs15 \additive \rtlch\fcs1 \af0 \ltrch\fcs0 \ul\cf17 \sbasedon10 \sunhideused \styrsid13251479 Hyperlink;}}{\*\listtable{\list\listtemplateid1050438670\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'00.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fbias0 \fi-360\li720\lin720 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'01.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li1440\lin1440 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'02.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li2160\lin2160 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'03.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li2880\lin2880 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'04.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li3600\lin3600 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'05.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li4320\lin4320 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'06.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li5040\lin5040 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'07.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li5760\lin5760 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'08.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li6480\lin6480 }{\listname ;}\listid667178746}{\list\listtemplateid204235190\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'00.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fbias0 \fi-360\li720\lin720 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'01.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li1440\lin1440 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'02.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li2160\lin2160 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'03.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li2880\lin2880 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'04.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li3600\lin3600 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'05.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li4320\lin4320 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'06.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li5040\lin5040 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'07.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li5760\lin5760 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'08.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li6480\lin6480 }{\listname ;}\listid1564834909}{\list\listtemplateid-246409782\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'00.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fbias0 \fi-360\li720\lin720 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'01.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li1440\lin1440 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'02.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li2160\lin2160 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'03.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li2880\lin2880 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'04.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li3600\lin3600 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'05.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li4320\lin4320 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698703\'02\'06.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li5040\lin5040 }{\listlevel\levelnfc4\levelnfcn4\leveljc0\leveljcn0\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698713\'02\'07.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-360\li5760\lin5760 }{\listlevel\levelnfc2\levelnfcn2\leveljc2\leveljcn2\levelfollow0\levelstartat1\lvltentative\levelspace0\levelindent0{\leveltext\leveltemplateid67698715\'02\'08.;}{\levelnumbers\'01;}\rtlch\fcs1 \af0 \ltrch\fcs0 \fi-180\li6480\lin6480 }{\listname ;}\listid2133787789}}{\*\listoverridetable{\listoverride\listid667178746\listoverridecount0\ls1}{\listoverride\listid2133787789\listoverridecount0\ls2}{\listoverride\listid1564834909\listoverridecount0\ls3}}{\*\rsidtbl \rsid203482\rsid805155\rsid873972\rsid928811\rsid1194947\rsid1337330\rsid1515604\rsid1591010\rsid2124042\rsid2777347\rsid2781756\rsid2971710\rsid3080871\rsid3238933\rsid3292075\rsid3429993\rsid3543040\rsid3694526\rsid3833442\rsid6319472\rsid7431516\rsid7474526\rsid7956788\rsid7961375\rsid8082896\rsid8203407\rsid8339360\rsid8412365\rsid9132121\rsid9190331\rsid9401763\rsid9448569\rsid9510076\rsid9992286\rsid10171803\rsid11219839\rsid11226378\rsid11469160\rsid11813154\rsid11862994\rsid12330342\rsid12347984\rsid12387273\rsid12478313\rsid12535127\rsid12989397\rsid12994432\rsid13251479\rsid13327146\rsid13377154\rsid13645330\rsid13903674\rsid14309010\rsid14701842\rsid14755991\rsid14886900\rsid15212770\rsid15889573\rsid16076162}{\mmathPr\mmathFont34\mbrkBin0\mbrkBinSub0\msmallFrac0\mdispDef1\mlMargin0\mrMargin0\mdefJc1\mwrapIndent1440\mintLim0\mnaryLim1}{\info{\operator Oliver Schulte}{\creatim\yr2015\mo10\dy21\hr14\min23}{\revtim\yr2015\mo10\dy24\hr14\min42}{\version24}{\edmins1344}{\nofpages24}{\nofwords4817}{\nofchars27461}{\nofcharsws32214}{\vern49803}{\*\saveprevpict}}{\*\xmlnstbl {\xmlns1 http://schemas.microsoft.com/office/word/2003/wordml}}\paperw12240\paperh15840\margl1800\margr1800\margt1440\margb1440\gutter0\ltrsect \ftnbj\aenddoc\trackmoves0\trackformatting1\donotembedsysfont0\relyonvml0\donotembedlingdata1\grfdocevents0\validatexml0\showplaceholdtext0\ignoremixedcontent0\saveinvalidxml0\showxmlerrors0\horzdoc\dghspace120\dgvspace120\dghorigin1701\dgvorigin1984\dghshow0\dgvshow3\jcompress\viewkind1\viewscale100\rsidroot3080871\outdisponlyhtml \fet0{\*\wgrffmtfilter 2450}\ilfomacatclnup0\ltrpar \sectd \ltrsect\linex0\sectdefaultcl\sftnbj {\*\pnseclvl1\pnucrm\pnstart1\pnindent720\pnhang {\pntxta .}}{\*\pnseclvl2\pnucltr\pnstart1\pnindent720\pnhang {\pntxta .}}{\*\pnseclvl3\pndec\pnstart1\pnindent720\pnhang {\pntxta .}}{\*\pnseclvl4\pnlcltr\pnstart1\pnindent720\pnhang {\pntxta )}}{\*\pnseclvl5\pndec\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl6\pnlcltr\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl7\pnlcrm\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl8\pnlcltr\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl9\pnlcrm\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}\pard\plain \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0 \rtlch\fcs1 \af0\afs20\alang4105 \ltrch\fcs0 \fs20\lang4105\langfe1033\cgrid\langnp4105\langfenp1033 {\rtlch\fcs1 \af5\afs36\alang1033 \ltrch\fcs0 \i\f5\fs36\cf1\lang1033\langfe1033\langnp1033\insrsid7956788 Dear Reviewers: Thank you for your time and help. This is the original decision email we received from the Guest Editors. Pointwise responses are inserted below.}{\rtlch\fcs1 \af0\afs38\alang1033 \ltrch\fcs0 \i\fs38\cf1\lang1033\langfe1033\langnp1033\insrsid3080871\charrsid7956788 \par }\pard \ltrpar\ql \li0\ri0\sa186\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0 {\rtlch\fcs1 \af0\afs38\alang1033 \ltrch\fcs0 \fs38\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0 {\rtlch\fcs1 \ab\af5\afs32\alang1033 \ltrch\fcs0 \b\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 From: }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 "Guest Editors of ILP 2014" <em@editorialmanager.com>\par }{\rtlch\fcs1 \ab\af5\afs32\alang1033 \ltrch\fcs0 \b\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 To: }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 "Zhensong Qian" <zqian@sfu.ca>\par }{\rtlch\fcs1 \ab\af5\afs32\alang1033 \ltrch\fcs0 \b\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Sent: }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Tuesday, September 22, 2015 2:28:17 AM\par }{\rtlch\fcs1 \ab\af5\afs32\alang1033 \ltrch\fcs0 \b\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Subject: }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Decision on your Manuscript #MACH-D-14-00294R1\par \par CC: jesse.davis@cs.kuleuven.be, jan.ramon@cs.kuleuven.be\par \par Dear Mr. Qian,\par \par The reports from the reviewers of your manuscript, "Fast Learning of Relational Dependency Networks", which you submitted to Machine Learning, have now been received.\par \par \par Based on the reviewers' reports your manuscript could }{\rtlch\fcs1 \ab\af5\afs32\alang1033 \ltrch\fcs0 \b\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 be accepted for publication contigent upon the incorporating the following minor revisions:}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par \par *Addressing the writing issues raised by the reviewers with respect to the clarity, claims and motivation.\par \par *Expanding the discussion of the proposed language bias and its pros and cons.\par \par *Providing additionally details about the comparison to the boosting experiments.\par \par Furthermore, please try to address the other minor issues noted by the reviewers. The manuscript will be checked by the reviewers to ensure that the following changes have been implemented. Given that this is a special issue, }{\rtlch\fcs1 \ab\af5\afs32\alang1033 \ltrch\fcs0 \b\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 we would like the revised version submitted within 4 weeks}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 . \par \par When preparing your revised manuscript, submit a point wise list of responses to the comments. \~Your list of responses should be uploaded as a separate file in addition to your revised manuscript. \par \par Please make sure to submit your editable source files (i. e. Word, TeX).\par \par In order to submit your revised manuscript electronically, please access the following web site:\par \par \~http://mach.edmgr.com/\par \par Your username is: zqian@sfu.ca\par \par To access your password, please complete the following steps:\par \par \~\~ 1.\~\~\~\~\~\~\~\~Open the URL: \~http://mach.edmgr.com/\par \~\~ 2.\~\~\~\~\~\~\~\~Click the LOGIN button on the banner.\par \~\~ 3.\~\~\~\~\~\~\~\~Click "Send Username/Password". \~\par \~\~ 4.\~\~\~\~\~\~\~\~Complete the required information (First Name, Last Name, Email Address). \par \~\~ 5.\~\~\~\~\~\~\~\~Click "Send Username and Password". \~\par \par Your Password will be sent to you by email.\par \par Click "Author Login" to submit your revision.\par \par We look forward to receiving your revised manuscript by \par \par Thank you.\par \par Best regards,\par \par Guest Editors of ILP 2014\par Guest Editor\par Machine Learning\par \par COMMENTS FOR THE AUTHOR:\par \par Reviewer #1: First, I would like to thank the authors for their diligent effort in making the paper better. The paper has certainly improved since its last iteration. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2777347 \par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0\pararsid2777347 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2777347 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2777347 Thank you for the positive feedback. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 We appreciate the time you have spent on our paper. Your comments help us improve and clarify the presentation.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2777347\charrsid2777347 \par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2777347 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Second, because I reviewed the last paper, I will focus my review mainly on the new version.\par \par As I mentioned earlier, the paper has improved quite a bit. The motivation is better. The experimental results make a bit more sense now given the discussion and the detailed response. I appreciate this.\par \par However, I still have a few concerns.\par \par The complexities that you have mentioned in Section 4.2 all assume a known fixed structure. Then it is linear in the number of parameters. However, when you are learning the structure itself, the problem becomes more complex. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9190331 \par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9190331 The new contribution in the paper is the algorithm for transforming a learned Bayesian network to a dependency network. So the complexity analysis you mention focus on that. We have added a new section (now 4.2) that reviews previous results about the complexity of Bayesian network structure learning. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9190331\charrsid9190331 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9190331 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 It is still unclear why checking acyclicity needs to be performed when learning a RDN which can have cycles. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9190331 \par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9190331 It\rquote s important to distinguish the template graph from the ground inference graph. See Figure 1, the discussion around Figure 1, and Neville 2007. In the case of a Bayesian network, it can happen that the template graph is acyclic but the ground graph is not, see Figure 1. We do not check acylicity in the ground graph. Acyclicity in the template graph leads to fast learning through closed-form parameter estimation. That is the same}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900  consequence that acyclicity has }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3292075 for propositional i.i.d. data, see the new Section 4 devoted to discussing Bayesian network learning.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9190331 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9190331\charrsid9190331 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 I see that the method is working well and so I am not questioning the validity. I still do not clearly see the arguments. The properties you are mentioning as implicit to relational data is the same for the propositional data. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3292075 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3292075\charrsid3292075 We expanded on the differences between relational and propositional BNs and DNs in the introduction. In the propositional case, there are no issues with cycles in the ground graph. (To the extent that there is a \u8220\'d2ground graph\u8221\'d3 for i.i.d. data, it is just a collection of disjoint copies of the template, one for each instance. See Neville 2007).\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Btw, you mention section 8.4 in your response but there is no such section.\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3292075 Sorry about that. That is Section 9. Section 10 is also relevant to comparing boosting and BN learning.\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3292075\charrsid3292075 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 In the relational model you say cycles are important. Yes, I agree. But a PRM or a BLP could have cycles in the relational level. They cannot have cycles in the ground (instantiated) network. So that argument seems a bit moot to me.\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3292075 This sounds complementary to what we are doing: we allow cycles in the final ground dependency network, but not in the }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid203482 template Bayesian network. See also the reply to the acylicity query.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3292075 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3292075\charrsid3292075 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 The experiments make much more sense. I appreciate that you made an effort to contact the authors of the other work to get results.\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid203482 Thanks!\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid203482\charrsid203482 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Just having 20 predicates and claiming to scale very well does not fly well. I agree that the current SRL data sets might not be large but they do not claim scale as much as you do. Tuffy/Deepdive for instance, can scale as well and they are not restricted to 19 predicates - particularly when working with NLP data. So please play this down and simply present yours as an alternative and efficient way. \~}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid16076162 \par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid16076162 Well, neither Tuffy nor Deepdive do structure learning. Deepdive allows a user to specify rules. We agree that learning such rules is an important potential application for our system. Also agreed that the number of (parametrized) random variables in our benchmarks are f}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 ar from what is needed for }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid16076162 NLP applications. We added a discussion of this application and  approaches to scaling up on the number of (parametrized) random variables in the Future work section. We added a comment on our datasets comparing them to previous SRL work in the Datasets section.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid16076162\charrsid16076162 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid16076162 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Please make the distinction to prior work clearly - stating that you report an average AUC across several predicates instead of the single ones reported earlier. The numbers look too low for the prior work, but since you did not get a reply from the authors, it is fine.\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 Added in Section 9.1.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900\charrsid14886900 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Over all, the paper has improved but the motivation is still a bit murky. It might make sense to simply state this as an alternative that is efficient due to several local search methods combined in a clever manner with data base access.\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9401763 That\rquote s a great}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid203482  way }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9401763 to phrase}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid203482  it. We used your wording in two places, and gave anonymous credit.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid203482\charrsid203482 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Reviewer #2: This is a resubmission. Consequently I will mainly focus on\par the issues raised in my previous review. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid15889573 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid15889573 Thank you for the}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3238933  helpful}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid15889573  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3238933 detailed comments, also in the previous review. Many of your points are closely connected, }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 so first}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3238933  we provide an overall reply, and then make specific comments about more specific points.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid15889573 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3238933 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9448569 The main concern seems to be that the difference in scalability between gradient boosting and Bayes net learning + DN}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14755991  conversion should be seen as due to a more restrictive language bias, rather than due to the difference between BN learning and boosting. At least that is a hypothesis worth exploring. - }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11469160 \line \line }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1337330 What we\rquote ve done in the paper is to move up the whole discussion of language bias and assessing structure learning into the introduction. This allows us to mention caveats, like that in our experiments we evaluate Bayes net learning with a certain language bias, vs. boosting with a different language. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12994432 We also note investigating the trade-offs by different language biases as an important direction for future work. Beyond what\rquote s in the paper, w}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1337330 e also make a few brief remarks in this reply. \line }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14755991 \par {\listtext\pard\plain\ltrpar \rtlch\fcs1 \af0\afs32 \ltrch\fcs0 \i\f5\fs32\cf1\insrsid14755991 \hich\af5\dbch\af0\loch\f5 1.\tab}}\pard \ltrpar\ql \fi-360\li720\ri0\nowidctlpar\wrapdefault\faauto\ls2\rin0\lin720\itap0\pararsid14755991 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14755991 We don\rquote t disagree. We certainly would not want to claim that Bayes net learning will be better than boosting on every dataset and for all patterns (rule types) of interest. You suggest that \par }\pard \ltrpar\ql \li360\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin360\itap0\pararsid14755991 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14755991\charrsid14755991 \par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0\pararsid14755991 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14755991 one should maybe just say that "the difference might be\par attributed to a different language bias. Nevertheless, the results show that\par the combination of relational BN learning and the BN-to-DN conversation makes \par it easy to scale relational structure learning". \par \par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0\pararsid13251479 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14755991\charrsid14755991 We\rquote re totally fine with that. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479 We think that there is lots of room for more structure learning methods in relational learning. For example, Kevin Murphy lists 40+ implemented Bayes net learning methods on his webpage }{\field{\*\fldinst {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12543222 HYPERLINK}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479  "}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479\charrsid13251479 http://www.cs.ubc.ca/~murphyk/Software/bnsoft.html}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479 " }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 {\*\datafield 00d0c9ea79f9bace118c8200aa004ba90b0200000003000000e0c9ea79f9bace118c8200aa004ba90b6600000068007400740070003a002f002f007700770077002e00630073002e007500620063002e00630061002f007e006d00750072007000680079006b002f0053006f006600740077006100720065002f0062006e0073006f00660074002e00680074006d006c000000}}}{\fldrslt {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \cs15\i\f5\fs32\ul\cf17\lang1033\langfe1033\langnp1033\insrsid13251479\charrsid3694526 http://www.cs.ubc.ca/~murphyk/Software/bnsoft.html}}}\sectd \ltrsect\linex0\sectdefaultcl\sftnbj {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479 . More have appeared in research papers. There is an equal number of Markov network learning methods. The number of methods for relational data is much less. If you consider structure learning systems only for which you can download code, we are aware of no more than 5}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3543040  (outside of MLNs)}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479 . So there seems to be space for a novel approach that gives you an implemented scalable system for learning dependency networks based on a log-linear model, even if it has limitations and }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 a }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479 restrictive }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 language bias.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1337330 \line }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9448569\charrsid9448569 \par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9448569 2. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479 The paper is}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397  about how to }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1337330 convert parametrized BNs to DNs. It\rquote s}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479 not about criticizing the boosting approach. On the contrary, we hope to give a boost to boosting. We really think that the best system will take advantage of the strengths of both approaches (or more). This is why we devote an entire section to how to combine the approaches. We list different strengths and weaknesses, and say that \u8220\'d2}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479\charrsid13251479 Given these fundamental differences, it is not surprising that our experiments show different strengths and limitations for each approach.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13251479 \u8221\'d3 \line \line As for the limitations of boosting, we didn\rquote t really think we were saying anything controversial. As for }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397 interpretability, the authors say themselves in the original boosting paper that they \u8220\'d2sacrifice interpretability\u8221\'d3. As for scalability, our initial motivation for this research came from conversations with Sriraam Natarajan}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1337330 , one of the developers of the boosting approach, in which}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1337330 he expressed concerns about scaling to large datasets}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397 . Informal conversations are no substitute for rigorous experiments, but it doesn\rquote t seem to us that we are saying anything that would surprise the boosting research group.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9448569 \par \par {\listtext\pard\plain\ltrpar \rtlch\fcs1 \af0\afs32 \ltrch\fcs0 \i\f5\fs32\cf1\insrsid12989397 \hich\af5\dbch\af0\loch\f5 3.\tab}}\pard \ltrpar\ql \fi-360\li720\ri0\nowidctlpar\wrapdefault\faauto\ls3\rin0\lin720\itap0\pararsid9132121 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397 We completely agree that experiments that explore the effects of various language biases on relational learning are important. We would like to know more about the tradeoff between increasing expressive power (and hence presumably predictive power), and computational cost for structure learning. But it\rquote s a question of the paper scope. The problem we wanted to address in this paper is how to leverage Bayesian network learning to obtain dependency networks that can be used in relational inferences. This does presuppose that Bayesian network learning is good enough that you would want to leverage it, and that may depend on the language bias used. But it\rquote s not the main contribution of the paper, which is the log-linear equation, which we formulated to be independent of language bias.\line There is also a chicken and egg problem with studying the language bias of }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid10171803 dependency network }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397 structure learning: to do that, you need to do define compatible inference models so you can }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid15212770 compare}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397  the predictive power of different learned models}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid15212770 . The current paper proposes a log-linear inference equation; together with Bayesian network structure learning, this }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12989397 makes it possible to explore different language biases even on large datasets.\line \line For what it\rquote s worth, we}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12535127  are already working on }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid10171803 extending }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11469160 Bayes net structure learning to include nodes with constants as well as variables. We submitted and received funding for a grant on this topic a couple of years ago. We published a paper on  how to score parametrized Bayesian networks with constants as well as first-order variables (AAAI Late Breaking track). We\rquote re about to publish a paper where we carefully evaluate our proposed score on a number of datasets. So we completely agree it\rquote s an important topic, we just think that it deserves another paper. Or  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12535127 m}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11469160 a}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12535127 ybe a}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11469160  few.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9448569\charrsid12989397 \par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9448569 \par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid15889573 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Thanks for changing the discussion of the Gibbs sampling\par approach. However, I think, the discussion is still\par not entirely correct. Page 11 says that the dependency\par networks are inconsistent. While this is true, it now\par gives the wrong impression that a joint distribution induced\par by the DN and evidence may not exists in general. \par I think the GSM idea should be discussed in more details \par to clarify this subtile point. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2971710 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2971710 This is a good point that supports our approach. We expanded on the discussion in Section 7 to summarize the results of Heckerman et al, and of Bengio, that show that the local conditional distributions can be used to define a unique stationary joint distribution for a sampling procedure.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2971710\charrsid2971710 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Furthermore, I would still very much appreciate the distinction of \par predicates and functors. Please note that a functor\par give rise to an infinite Herbrand base, which means that\par we cannot learn the structure of the RDN as we would\par need relational regression trees with recursion that do not\par really exists to represent an infinite (ground) RDN. \par So, it is not about whether you \par refer to other texts or not but whether it makes sense\par in context of the paper, namely structure learning of relational\par models. This was not really considered by Poole and Kimmig et al. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9132121 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9132121\charrsid9132121 Well, Kimmig et al. say the following in Section 2.2.5.\par \par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0\pararsid9132121 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9132121\charrsid9132121 \u8220\'d2Formulas containing functions of arity at least one have infinitely many groundings, which is often undesirable when using FOL to specify SRL models. One way to avoid this is to only consider groundings where variables are replaced with constants in all pos- sible type-consistent ways, e.g., Author(p1,X) can be grounded to Author(p1,r1),\par Author(p1, r2), Author(p1, r3) and Author(p1, r4) in our example.\u8221\'d3\par }\pard \ltrpar\ql \li0\ri0\nowidctlpar\wrapdefault\faauto\rin0\lin0\itap0 {\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9132121\charrsid3543040 As for our learning algorithm and log-linear equation, all it requires is being able to compute the proportion of domain elements that satisfy a first-order formula. The proportion concept can be extended to infinite domains as shown by Halpern and Bacchus. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7431516\charrsid3543040 For collective inf}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 erence, as in Gibbs sampling, reasoning over an infinite set of ground terms}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7431516\charrsid3543040  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 is indeed a challenge}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7431516\charrsid3543040 . We have added remarks }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 about the finite domain}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11813154\charrsid3543040  in Sec.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7431516\charrsid3543040  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14886900 4.2 for learning, with a reference to Kimmig et al\rquote s discussion.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9132121\charrsid3543040 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9132121 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Likewise, it is not enough to say that a similar language bias \par was used by previous papers. You should explain the language bias used \par and why it might be actually useful to use it. So, why is\par it useful to have all predicates variablized? At least you have to \par say that is is an assumption and if different ground atoms over\par the same predicate require different wighted rules then this\par may fail. Without doing so, you paper gives the wrong impression that\par the language biased used is universal, a claim that is not proven \par at all. That is, it is not enough to just cite previous work in particular\par given that [9] is really already more than 15 years old and [6]\par is not really focussing on structure learning; at least it is\par not discussing state-of-the-art structure learning approach; forbidding\par constants was more a question of computational efforts than \par of what we would like to have. \par \par Hence, the discussion of the language bias has to be extended.\par Just saying others have done this is not enough. The pros and cons\par have to be discussed. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3543040 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid10171803 We extended the discussion as explain}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid8082896 ed in our introductory remarks above.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3543040\charrsid3543040 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par The discussion should also reflect on the boosting approach. Here, \par the tree may indeed provide different rules for different ground atoms\par over the same predicate. In turn, one should compare to a boosting approach\par where all internal tests consider variablized/maximal queries only, \par i.e., no equality constraints resp. substitutions. This would considerably\par reduce the hypothesis space of the tree learning and in turn \par explain why currently the learning times for some of the boosting\par setting are significantly larger. BTW, I was really trying to point\par this out already in my previous review, and hence, I was expected some\par more details. Unfortunately, this is not the case. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid10171803 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid10171803 Sorry we missed that. We tried to address everything, but somehow we didn\rquote t get that\rquote s what you had in mind. Well, unfortunately the original boosting papers did not investigate what happens when you restrict the internal tests to variables only. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid928811 We checked and there is no setting in the implemented Boostr system for that. The best way forward seems to extend Bayesian network learning to include}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid8082896  constants, as discussed above.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid10171803\charrsid10171803 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Thanks for extending the introduction. The discussion of the intuition\par underlying your approach already there really helps getting into\par the paper. The additional Section 6 on BN learning helps even more. \par I also really like the discussion of what makes relational learning\par different to propositional learning when it comes to dependency networks. \par This has really greatly improved the paper. That. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid928811 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid928811 Thanks! }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid928811\charrsid928811 {\field{\*\fldinst SYMBOL 74 \\f "Wingdings" \\s 16}{\fldrslt\f14\fs32}}}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid928811\charrsid928811 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par The evaluation setting is now clear, too. Still, combining back to the\par language bias, one should maybe just say that "the difference might be\par attributed to a different language bias. Nevertheless, the results show that\par the combination of relational BN learning and the BN-to-DN conversation makes \par it easy to scale relational structure learning". \par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid928811 We added caveats and placed them in the introduction as well as in the comparison of boosting vs. BNs.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid928811\charrsid928811 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par To summarise, the paper has definitely been improved. It provides\par the intuition underlying the present approach and also its details. Thanks. \par \par However, the pros and cons of the language bias and in turn of the proposed\par structure learning approach has to be discussed in more details. This is\par of mainly editorial work, and I consider this as minor revision although \par another reviewing round is required. \~\par \par minor:\par \par (page 4) "Kersting and *De Raedt*" and also in the corresponding reference. \par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11226378 Oops. Fixed.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871\charrsid11226378 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11226378 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Reviewer #4: The paper introduces learning Relational Dependency Networks by first learning a relational Bayesian network and then transforming into a relational dependency network. The experimental setup show that the proposed approach scales well with the number of data tuples, but not with the number of predicates. The authors also claim that their comparative predictive performance stems from more powerful features that Bayesian\par network learning finds in relational datasets. \par The idea of the paper is simple and compelling, but needs more technical details and text restructuring for purpose of easier reading before publishing.\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14701842 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14701842 Thank you for the positive, indeed encouraging, assessment. We appreciate the time you took to write down your comments, especially that they provide very specific guidance for changes. We agree that these are improvements and have aimed to implement your suggestions. The details are given point-by-point below.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14701842\charrsid14701842 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 1:\par Page 3: When talking about local probability distributions the authors say: "a conditional probability in a dependency network efficiently specifies the probability of a node value given an assignment to ALL other nodes". Even though this is true, the authors should mention "feature selection" by means which we have CPDs where the set of parents renders the child node conditionally independent of all other variables. Because it's more known that CPDs represent a probability distribution of a node conditioned on its parent nodes (not all nodes).\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Reference to feature selection added in Section 2.1.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871\charrsid3080871 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 2:\par Page 3: Authors sometimes use Bayesian network when they mean Parametrized Bayesian network. E.g., in Section 2.2: " A Bayesian network structure or relational dependency network structure augmented with appropriate ..."\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1194947 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1194947 We have added \u8220\'d2Parametrized\u8221\'d3 throughout section 2.2. After this section, almost all references are to parametrized Bayesian networks, so we start calling them simply \u8220\'d2Bayesian networks\u8221\'d3 for brevity. We state this convention explicitly}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2781756  at the beginning of Section 3}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1194947 .}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1194947\charrsid1194947 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 3:\par Page 5: I think it would be good to follow the program flow in Figure 2 when organising sections. For example it would be good to first start on how to learn Bayesian networks before introducing details of the conversion (BN-to-DN parameter conversion). In this way it would be easier to follow the pipeline introduce by the authors.\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2781756 We reordered}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13645330  the sections to follow the pipeline of figure 2.\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13645330\charrsid13645330 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Comment 4:\par Page 5: (also Page 6) Authors use the citation [37, Ch.14.5.2], while I don't see how this particular chapter in Russel and Norvig book refers to it. The chapter is about MCMC and Gibbs sampling while the authors refer to BN product formula and log-conditional probabilities. Even though of course this is relevant for MCMC, it's not the main reference point. Maybe authors should just refer to Chapter 14 as a general reference point to probabilistic reasoning.\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2781756 In the propositional case, the local probability distribution can be found as we say \u8220\'d2}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2781756\charrsid2781756 solve for the local probability distributions given Bayesian network parameters~\\cite[Ch.14.5.2]\{Russell2010\}.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2781756 \u8221\'d3. Section 14.5.2. in the AMAI textbook gives the solution that we report later in the section. As you say, the reason why Russell and Norvig report this solution in the chapter on Gibbs sampling is because the local distribution is used in Gibbs sampling. We could reprove the solution for the propositional case, but it is a known exercise. The Russell reference states the result, it it was the most prominent reference we could find. Also, (variants of) Gibbs sampling are the standard way to do collective inference in dependency networks, so a reference to Gibbs sampling seems appropriate. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2781756\charrsid2781756 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Comment 5:\par Page 8: I failed to see how the random selection happens in the Equation (1). Maybe some notations is missing?\par "The inner sum of Equation (1) computes the expected log-conditional probability for a family with child node U , when we randomly select a relevant grounding of the first-order variables in the family."\par This should already be specified when introducing the formula.\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9510076 We introduced extra notation for defining the concept of \u8220\'d2expected log-conditional probability\u8221\'d3. Using this notation, we state }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12330342 the random selection concept as a formal proposition. The proof is given in the appendix}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid8203407  in Section 13.1}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12330342 . We hope }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid8203407 that a formal proposition}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12330342  makes the statement of the claim precise and clear, while at the same time presenting formal details only to readers who are interested. Since the statement of the proposition takes more space than our previous remark, we moved it to the end of Section 6.1.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2124042\charrsid9510076 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid2124042 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Comment 6:\par Pseudocode: I think it would be easier to read the text if the authors combined explaining the pieces of the pipeline with the pseudocode they provide. E.g., BN-to-DN structure conversion (Algorithm 1),\par BN-to-DN parameter conversion (Algorithm 2). It's better than to referring to it only in Section 4.1.\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7961375 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7961375 We expanded section 3 to include and discuss Algorithm 1, the BN-to-DN structure conversion. Algorithm 2 appears immediately after the equation that it implements.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7961375\charrsid7961375 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 7:\par Log-Linear proportion equation: I have a feeling that this equation is connected to the MLN equation for probability distributions where we only have that \par weights of features correspond to the CPDs and instead of using the count of true groundings, you're using the proportion (of which I understand the motivation). Also MLNs have the normalization cnostant. Might not be very important but this correspondendce could be maybe mentioned.\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7961375 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid6319472 You are right about the relationship to MLNs. We discussed this in the related work section (now Section 13). We added a forward pointer in Section 6 to the discussion in Section 13. We also added a reference in Section 13 to a workshop paper that compares proportions vs. counts empirically.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7961375\charrsid6319472 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 8: How does the parametrized Bayesian network relate to Relational Bayesian Networks (RBNs) from the representational point of view?\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7474526\charrsid1515604 Kimmig et al. to a great job of comparing the different formalisms using parametrized random variabl}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1515604\charrsid1515604 es. We added a mention of this aspect of their }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7474526\charrsid1515604 survey under related work in Section 11. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1515604\charrsid1515604 Briefly, RBNs a}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3429993 re more expressive than our }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1515604\charrsid1515604 Parametrized BNs. First, in defining the conditional distribution of children given parents, one can use Boolean combinations of the parents (like or). Second, they allow quite general combining rules for multiple instantiations. Using proportions corresponds to using the geometric mean as a combining rule, as we mentioned in Section 11. In recent work, Poole and his collaborators have shown that log-linear models actually have surprising expressive power: by definining the right parametrized random variables, they can express various combining rules. We refer to this in what is now Section 6. This means that our log-linear equation is actually more expressive than it may seem. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1515604 \line There is no structure learning algorithm for RBNs that we know of, certainly no implementation.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14309010  We}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13903674  did a literature search and}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14309010  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13327146 checked}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12347984  this}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13327146  with Manfred Jaeger }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13903674 too}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13327146  (the inventor of RBNs).}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid7474526\charrsid1515604 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 9: I am indeed quite confused with emphasizing the learning of Bayesian networks which is a propositional framework. Authors are all the time learning relational or lifted models. It's not that important, but it's confusing when propositional frameworks are mixed with relational. In one of the cited papers: "Structure Learning for Markov Logic Networks with Many Descriptive Attributes",Khosravi et al., they say they develop an algorithm "...for learning the structure of a parametrized BN". I do however understand that the propositional BN learner represents just one procedure in the algorithm what makes the entire approach lifted. One specific confusion is when the authors say: "This\par is a state-of-the-art Bayesian network structure learning..." in Section 6.\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1515604 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1515604 Indeed, all structure learning algorithms in this paper are for relational data. We think this is easier to follow now because we moved BN structure learning for relational data up as suggested in your comment }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14309010 3. We stated exactly when we start using \u8220\'d2Bayesian network learning\u8221\'d3 instead of \u8220\'d2parametrized Bayesian network learning\u8221\'d3, see your comment 2. As for the particular passage that you mention, we changed it to read \u8220\'d2}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14309010\charrsid14309010 state-of-the-art parametrized Bayesian network structure learning algorithm for relational data.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14309010 \u8221\'d3 The changes we made in response to reviewer #1s comments should help with this too.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1515604\charrsid1515604 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 10: \par I am also a bit confused by the motivation. I do understand that the experiments show improvement when learning in proposed way, but I also want to understand why. The authors claim that the main cost of relational learning is the fact that we have to calculate the instantiations of complex chains of queries. However, in your BN-to-DN conversion you are also evaluating these queries (finding the number of true instantiations of features in the data), and you do this for each ground atom. For this I guess you also need data access. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14309010 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid9992286 We do need data access, but only to the sufficient statistics, not to each ground atom. While looping over all ground instances is one way to get sufficient statistics = instantiation counts, you can get faster answers by doing in effect block access. This is the main idea behind Poole\rquote s \u8220\'d2lifted first-order inference\u8221\'d3 that uses parfactors. We expanded on the section }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154 in our paper that you refer to (now 4.2), by giving references to some of the relational counting techniques. These include sorting and pivoting (databases + Graefe, Fayyad), the tupleID propagation (Ji}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3429993 awei Han and collaborators),  the fast Moebius transform, and the very recent reduction to computing the partition function of a Markov network (}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid8412365 Venugopal et al. AAAI 2015).}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154  The changes we made in response to reviewer #1 should also clarify the issue of counting vs. looping over all ground instances, like other relational dependency network learning methods do.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid14309010\charrsid9992286 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 11:\par It would be good to include the number of random variables for each data set, and not only PRVs. Of course, the more PRVs or predicates, random variables (ground atoms) there are. You are also having the descriptive attribute for which the number of groundings increases with the range they have. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154 See comment 12.}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154\charrsid13377154 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 12:\par I am not sure what kTuple means in Table 2. It seems not to be introduced earlier.\par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154 kTuple = 1000 Tuples. We changed kTuple to be }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid873972 #Tuples/1000}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154  Tuples, hopefully }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid873972 this is }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154 clearer. Does this address your concern in comment 11? It gets at the same thing, the number of instances as opposed to the number of (parametrized random variables). It\rquote s not exactly the same thing because a tuple of length m contains m ground atoms. The advantage of measuring database size in terms of data tuples is that this is common in database literature. We can change this to random variables if it\rquote s not what you had in mind. }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid13377154\charrsid13377154 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 13:\par When learning relational dependency networks, a local model can be learned for each predicate separately (if one doesn't care about consistency), while in case of Bayesian networks this cannot be done. Therefore, learning possibly (inconsis}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1515604 t}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 ent) relational dependency networks \par can be done fast in parallel. \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid805155 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid805155 This is an interesting point related to the comments of reviewer 1. Briefly, this is true and it\rquote s a reason why the scalability advantages of learning a parametrized Bayesian network arise mainly from being able to deal with many rows/tuples/instances, rather than many columns/parametrized random variables. We mentioned this when we discussed the learning times of the respective algorithms (now section 8.1). \par \par The parallelization gain, however, is limited because when learning the Markov blanket of each parametrized random variable node, the dependency network learner still has to consider all }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11862994 other nodes for each target node. But we think there is a potential big win here if you first use the BN learning to find the Markov blanket of a target node, then use boosting to learn a log-linear conditional probability model. For nodes }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3833442 whose BN Markov blankets contain disjoint features}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11862994 , the boosting could indeed be done in parallel on disjoint datasets. We added this nice point to our dis}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12478313 cussion in Section 10. We hope it\rquote s}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11862994  okay}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11219839  to incorporate your observation}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid12478313  in this way}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid11862994 . We gave you anonymous credit in a footnote.  }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid805155\charrsid805155 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par Comment 14:\par Related to this it would be good to explain better this number of predicates vs. number of data tuples. How are they related? Maybe authors should provide domain sizes for each object class and range of each predicate (for descriptive attributes).\par \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1591010 We listed explicitly the object classes and their domain sizes. There are quite a number of predicates, so instead of listing all, we listed the average arity of the predicates for each object class. Please note that these are standard datasets and we have made them all available on-line in various formats. So a reader who wants to know more information about the datasets can easily find the details. In terms of explaining the experimental results, we found that the most explanatory quantities are the number of rows (tuples) and columns + relationship tables (parametrized random variables).}{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \i\f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1591010\charrsid1591010 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid1591010 \par }{\rtlch\fcs1 \af5\afs32\alang1033 \ltrch\fcs0 \f5\fs32\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 Comment 15:"While relational counting is not an easy problem, researchers have developed efficient solutions.\par The important point for our experiments is that model evaluation by counting, which is most of what Bayes nets require, is much faster than iteratively computing model predictions for the ground facts in the input database ...". \par I have to say that I am not sure how this reflects in Equation 1, as clearly the formula goes over all ground facts. I think authors should really focus on explaining or illustrating this because this is the key point of the success of their approach.\par }{\rtlch\fcs1 \af0\afs38\alang1033 \ltrch\fcs0 \fs38\cf1\lang1033\langfe1033\langnp1033\insrsid3080871 \par }{\rtlch\fcs1 \af0\afs38\alang1033 \ltrch\fcs0 \fs38\cf1\lang1033\langfe1033\langnp1033\insrsid12387273 Please see comment 9.\par }{\*\themedata 504b0304140006000800000021009be8704ffc0000001c020000130000005b436f6e74656e745f54797065735d2e786d6cac91cb6ac3301045f785fe83d0b6d872ba28a5d8cea28f5d1f8bf40306796c8bd823214d42f2f71d3b2e941202856e04d2ccbdf7cca85c1fc641ed3126e7a9d2abbcd00ac9fac65157e9cfcd4b76af5562a006064f58e92326bdaeafafcacd316052a2a654e99e393c18936c8f23a4dc0724a9b43e8ec0728d9d0960b7d0a1b92d8a3b633d3112673c79e8ba7cc2167603abe7833c9f4844aed5e3a96f8aaa348430380b2ca066aa9ab3ba8843ba20dc53f38b2e5bc87251cee6a97721dd2c09efb29ae81a541f10f90d46e1302c43e2cff31548468bf965e633d1be6d9dc5c6dbdd28ebc867e3c5ec4f00abff89fece34f3dfd65f000000ffff0300504b030414000600080000002100a5d6a7e7c0000000360100000b0000005f72656c732f2e72656c73848fcf6ac3300c87ef85bd83d17d51d2c31825762fa590432fa37d00e1287f68221bdb1bebdb4fc7060abb0884a4eff7a93dfeae8bf9e194e720169aaa06c3e2433fcb68e1763dbf7f82c985a4a725085b787086a37bdbb55fbc50d1a33ccd311ba548b63095120f88d94fbc52ae4264d1c910d24a45db3462247fa791715fd71f989e19e0364cd3f51652d73760ae8fa8c9ffb3c330cc9e4fc17faf2ce545046e37944c69e462a1a82fe353bd90a865aad41ed0b5b8f9d6fd010000ffff0300504b0304140006000800000021006b799616830000008a0000001c0000007468656d652f7468656d652f7468656d654d616e616765722e786d6c0ccc4d0ac3201040e17da17790d93763bb284562b2cbaebbf600439c1a41c7a0d29fdbd7e5e38337cedf14d59b4b0d592c9c070d8a65cd2e88b7f07c2ca71ba8da481cc52c6ce1c715e6e97818c9b48d13df49c873517d23d59085adb5dd20d6b52bd521ef2cdd5eb9246a3d8b4757e8d3f729e245eb2b260a0238fd010000ffff0300504b030414000600080000002100215aa28421070000db1d0000160000007468656d652f7468656d652f7468656d65312e786d6cec594f6f1b4514bf23f11d467b2fb113274da23a55ecd80db469a3d82dea71bc3bf64e33bbb39a1927f10db547242444411ca8c48d03022ab51297f269024550a47e05deccecae77e271939400153487d63bfb7b6fdefbbd3ff367af5c3d4a183a2042529e3683fa7bb5009134e4114d47cde076bf7b69354052e134c28ca7a4194c880cae6ebcfbce15bcae62921004f2a95cc7cd20562a5b5f5890210c63f91ecf480aef865c2458c1a3182d44021f82de842d2cd66a2b0b09a66980529c80da5bc3210d09ea6b95c146a1bcc3e03155520f844cf4b46ae248186cb45fd70839916d26d00166cd00e689f8619f1ca900312c15bc680635f3172c6c5c59c0ebb9105373642b725df397cbe502d1fea299538c06e5a4f56e63edf256a9df00989ac5753a9d76a75eea33001c86e0a9b5a5aab3d15dadb70a9d1590fd39abbb5d5bae355c7c45ffd28ccd6bad566b792db7c52a3520fbb331835fadad3436171dbc0159fcf20cbed1da6cb7571cbc0159fcca0cbe7b796da5e1e20d286634dd9f41eb8076bbb9f61232e46cdb0b5f05f86a2d874f51900d6576e929863c55f3722dc1f7b8e8024003195634456a9291210e218bdb98d181a07a02bc4e70e58d1d0ae5cc909e0bc950d04c35830f320c1531d5f7f2d9772f9f3d41c7f79f1edffff1f8c183e3fb3f58458ed4364e4755a917df7cfac7a38fd0ef4fbe7ef1f0733f5e56f1bf7cfff1cf3f7de60742f94ccd79fec5e35f9f3e7efee527bf7dfbd003df14785085f7694224ba490ed11e4fc031c38a6b391988f349f4634cab129be948e214eb593cfa3b2a76d0372798610fae455c06ef08681f3ee0b5f13dc7e05e2cc62a8fb7e3d9f53871803b9cb316175e16aeebb92a34f7c7e9c83fb91857717b181ff8e66ee3d4896f679c41dfa43e95ed983866ee329c2a3c22295148bfe3fb8478f8ba4ba9c3eb0e0d05977ca8d05d8a5a987a29e9d381934d53a16d9a405c263e0321de0e373b77508b339fd75be4c045425560e631be4f9843e3353c5638f1a9ece3845509bf8155ec33b237116115d7910a223d228ca34e44a4f4c9dc12e06f25e8d7a175f8c3bec326898b148aeefb74dec09c57915b7cbf1de324f3617b348dabd8f7e53ea42846bb5cf9e03bdcad10fd0c71c0e9dc70dfa1c409f7e9dde0361d39264d1344bf190b4f2caf11eee46f6fc286989856034ddde9d5094d5fd5b813e8dbb9e317d7b8a1553effea91c7ee37b5656f0209be9ad93ed1a8e7e14eb6e73617117df3bbf3161ea7bb040a6276897adb9cdf36e7e03fdf9ce7d5f3c5b7e469178606adb74c76a36db6ddc9dc5df79032d65313466e48b3f196b0f6445d18d472e6c449ca535816c34f5dc93081831b096c6490e0ea43aae25e8c33d8b4d703ad642473d52389322ee1b06886bdba351e36feca1e3597f521c4760e89d50e8fecf0921e2ece1aa51a63d5c81c688b8996b482b34eb67439570abebdce64756dd49967ab1bd34c5374662b5dd6149b4339505eba0683259bb0a941b015029657e0ccafa786c30e6624d2bcdb1815613151f87b42947b6d1d8971446c889ce10a9b7513bb228566fcd3eed91c391f9b256b40dae94698b4989f3f6724b950302519044f56134babb5c55274d80cd69617970314e2ac190ce1980b3f930c8226f53610b311dc15854ad8ac3db5164d914e3d5ef367551d6e2ee6148c53c699906a0bcbd8c6d0bcca43c5523d93b57f71b9a193ed621cf03493b359b1b40a29f2af5901a176434b864312aa6ab02b239a3bfb9877423e5644f4e2e8100dd858ec61083f70aafd89a884db0a53d0fa01aed634dbe695db5bf34e53bdd032383b8e5916e3bc5beaab99a2e22cdcf493d206f354310f7cf3da6e9c3bbf2bbae22fca956a1affcf5cd1cb015c1e2c453a0221dcec0a8c74a534032e54cca10b65310dbb02d67dd33b205be07a165e03f970bf6cfe17e440ff6f6bceea30650d6740b547474850584e542c08d985b664b2ef1465f57ce9b12a59aec86454c55c9959b307e480b0beee812bba070728865437dd246f03067732ffdce7bc820623bd47a9d69bd3c9caa5d3d6c03fbd71b1c50c4e9dd84be8fc2df82f4d2c57f7e9ea67e58d78b146561dd12fa6bba4465115cee2b7b6964ff59a269c6501aeacb5b663cd78bcb85c1807519cf51806cbfd4c06574048ff03eb1f1521b31f2bf482dae77bd05b117c7bb0fc21c8ea4bbaab4106e906697f0d60df63076d32695596da7ce7a3592b16eb0bdea896f39e205b5b7696789f93ec7213e54ee7d4e245929d33ec706dc7e6520d913d59a230342cce212630e62b57f543141fdc83406fc195ff98d94f533283275307d9ae30d935e0d124ffc9a45d706dd6e9338c46b2748f0c118d8e8af347c9842d21fb79a4d8221bb416d389560a2ef90e0dae608ed7a276b52c85174f172e25ccccd0b24b617397e653001fc7f2c6ad8f7680b74dd67aad8bab608aa57f85b23318efa7cc7bf2392b65f6a0f8ca40bd0665eae8d594e54c0179b389079f370586a357cff45f58746ca69b94ddf8130000ffff0300504b0304140006000800000021000dd1909fb60000001b010000270000007468656d652f7468656d652f5f72656c732f7468656d654d616e616765722e786d6c2e72656c73848f4d0ac2301484f78277086f6fd3ba109126dd88d0add40384e4350d363f2451eced0dae2c082e8761be9969bb979dc9136332de3168aa1a083ae995719ac16db8ec8e4052164e89d93b64b060828e6f37ed1567914b284d262452282e3198720e274a939cd08a54f980ae38a38f56e422a3a641c8bbd048f7757da0f19b017cc524bd62107bd5001996509affb3fd381a89672f1f165dfe514173d9850528a2c6cce0239baa4c04ca5bbabac4df000000ffff0300504b01022d00140006000800000021009be8704ffc0000001c0200001300000000000000000000000000000000005b436f6e74656e745f54797065735d2e786d6c504b01022d0014000600080000002100a5d6a7e7c0000000360100000b000000000000000000000000002d0100005f72656c732f2e72656c73504b01022d00140006000800000021006b799616830000008a0000001c00000000000000000000000000160200007468656d652f7468656d652f7468656d654d616e616765722e786d6c504b01022d0014000600080000002100215aa28421070000db1d00001600000000000000000000000000d30200007468656d652f7468656d652f7468656d65312e786d6c504b01022d00140006000800000021000dd1909fb60000001b0100002700000000000000000000000000280a00007468656d652f7468656d652f5f72656c732f7468656d654d616e616765722e786d6c2e72656c73504b050600000000050005005d010000230b00000000}{\*\colorschememapping 3c3f786d6c2076657273696f6e3d22312e302220656e636f64696e673d225554462d3822207374616e64616c6f6e653d22796573223f3e0d0a3c613a636c724d617020786d6c6e733a613d22687474703a2f2f736368656d61732e6f70656e786d6c666f726d6174732e6f72672f64726177696e676d6c2f323030362f6d61696e22206267313d226c743122207478313d22646b3122206267323d226c743222207478323d22646b322220616363656e74313d22616363656e74312220616363656e74323d22616363656e74322220616363656e74333d22616363656e74332220616363656e74343d22616363656e74342220616363656e74353d22616363656e74352220616363656e74363d22616363656e74362220686c696e6b3d22686c696e6b2220666f6c486c696e6b3d22666f6c486c696e6b222f3e}{\*\latentstyles\lsdstimax276\lsdlockeddef0\lsdsemihiddendef1\lsdunhideuseddef1\lsdqformatdef0\lsdprioritydef99{\lsdlockedexcept \lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority0 \lsdlocked0 Normal;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 1;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 2;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 3;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 4;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 5;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 6;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 7;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 8;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 9;\lsdpriority39 \lsdlocked0 toc 1;\lsdpriority39 \lsdlocked0 toc 2;\lsdpriority39 \lsdlocked0 toc 3;\lsdpriority39 \lsdlocked0 toc 4;\lsdpriority39 \lsdlocked0 toc 5;\lsdpriority39 \lsdlocked0 toc 6;\lsdpriority39 \lsdlocked0 toc 7;\lsdpriority39 \lsdlocked0 toc 8;\lsdpriority39 \lsdlocked0 toc 9;\lsdqformat1 \lsdpriority35 \lsdlocked0 caption;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority10 \lsdlocked0 Title;\lsdpriority1 \lsdlocked0 Default Paragraph Font;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority11 \lsdlocked0 Subtitle;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority22 \lsdlocked0 Strong;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority20 \lsdlocked0 Emphasis;\lsdsemihidden0 \lsdunhideused0 \lsdpriority59 \lsdlocked0 Table Grid;\lsdunhideused0 \lsdlocked0 Placeholder Text;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority1 \lsdlocked0 No Spacing;\lsdsemihidden0 \lsdunhideused0 \lsdpriority60 \lsdlocked0 Light Shading;\lsdsemihidden0 \lsdunhideused0 \lsdpriority61 \lsdlocked0 Light List;\lsdsemihidden0 \lsdunhideused0 \lsdpriority62 \lsdlocked0 Light Grid;\lsdsemihidden0 \lsdunhideused0 \lsdpriority63 \lsdlocked0 Medium Shading 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority64 \lsdlocked0 Medium Shading 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority65 \lsdlocked0 Medium List 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority66 \lsdlocked0 Medium List 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority67 \lsdlocked0 Medium Grid 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority68 \lsdlocked0 Medium Grid 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority69 \lsdlocked0 Medium Grid 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority70 \lsdlocked0 Dark List;\lsdsemihidden0 \lsdunhideused0 \lsdpriority71 \lsdlocked0 Colorful Shading;\lsdsemihidden0 \lsdunhideused0 \lsdpriority72 \lsdlocked0 Colorful List;\lsdsemihidden0 \lsdunhideused0 \lsdpriority73 \lsdlocked0 Colorful Grid;\lsdsemihidden0 \lsdunhideused0 \lsdpriority60 \lsdlocked0 Light Shading Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority61 \lsdlocked0 Light List Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority62 \lsdlocked0 Light Grid Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority65 \lsdlocked0 Medium List 1 Accent 1;\lsdunhideused0 \lsdlocked0 Revision;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority34 \lsdlocked0 List Paragraph;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority29 \lsdlocked0 Quote;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority30 \lsdlocked0 Intense Quote;\lsdsemihidden0 \lsdunhideused0 \lsdpriority66 \lsdlocked0 Medium List 2 Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority70 \lsdlocked0 Dark List Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority71 \lsdlocked0 Colorful Shading Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority72 \lsdlocked0 Colorful List Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority73 \lsdlocked0 Colorful Grid Accent 1;\lsdsemihidden0 \lsdunhideused0 \lsdpriority60 \lsdlocked0 Light Shading Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority61 \lsdlocked0 Light List Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority62 \lsdlocked0 Light Grid Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority65 \lsdlocked0 Medium List 1 Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority66 \lsdlocked0 Medium List 2 Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority70 \lsdlocked0 Dark List Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority71 \lsdlocked0 Colorful Shading Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority72 \lsdlocked0 Colorful List Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority73 \lsdlocked0 Colorful Grid Accent 2;\lsdsemihidden0 \lsdunhideused0 \lsdpriority60 \lsdlocked0 Light Shading Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority61 \lsdlocked0 Light List Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority62 \lsdlocked0 Light Grid Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority65 \lsdlocked0 Medium List 1 Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority66 \lsdlocked0 Medium List 2 Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority70 \lsdlocked0 Dark List Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority71 \lsdlocked0 Colorful Shading Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority72 \lsdlocked0 Colorful List Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority73 \lsdlocked0 Colorful Grid Accent 3;\lsdsemihidden0 \lsdunhideused0 \lsdpriority60 \lsdlocked0 Light Shading Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority61 \lsdlocked0 Light List Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority62 \lsdlocked0 Light Grid Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority65 \lsdlocked0 Medium List 1 Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority66 \lsdlocked0 Medium List 2 Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority70 \lsdlocked0 Dark List Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority71 \lsdlocked0 Colorful Shading Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority72 \lsdlocked0 Colorful List Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority73 \lsdlocked0 Colorful Grid Accent 4;\lsdsemihidden0 \lsdunhideused0 \lsdpriority60 \lsdlocked0 Light Shading Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority61 \lsdlocked0 Light List Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority62 \lsdlocked0 Light Grid Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority65 \lsdlocked0 Medium List 1 Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority66 \lsdlocked0 Medium List 2 Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority70 \lsdlocked0 Dark List Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority71 \lsdlocked0 Colorful Shading Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority72 \lsdlocked0 Colorful List Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority73 \lsdlocked0 Colorful Grid Accent 5;\lsdsemihidden0 \lsdunhideused0 \lsdpriority60 \lsdlocked0 Light Shading Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority61 \lsdlocked0 Light List Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority62 \lsdlocked0 Light Grid Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority65 \lsdlocked0 Medium List 1 Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority66 \lsdlocked0 Medium List 2 Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority70 \lsdlocked0 Dark List Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority71 \lsdlocked0 Colorful Shading Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority72 \lsdlocked0 Colorful List Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdpriority73 \lsdlocked0 Colorful Grid Accent 6;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority19 \lsdlocked0 Subtle Emphasis;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority21 \lsdlocked0 Intense Emphasis;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority31 \lsdlocked0 Subtle Reference;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority32 \lsdlocked0 Intense Reference;\lsdsemihidden0 \lsdunhideused0 \lsdqformat1 \lsdpriority33 \lsdlocked0 Book Title;\lsdpriority37 \lsdlocked0 Bibliography;\lsdqformat1 \lsdpriority39 \lsdlocked0 TOC Heading;}}{\*\datastore 010001000200000008000000556e6b6e6f776e00000000000000000000060000d0cf11e0a1b11ae1000000000000000000000000000000003e000300feff090006000000000000000000000001000000010000000000000000100000feffffff00000000feffffff0000000000000000fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffdfffffffeffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff5200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002000500ffffffffffffffffffffffff0c6ad98892f1d411a65f0040963251e50000000000000000000000000d71ee66590ed101feffffff00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000ffffffffffffffffffffffff0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000ffffffffffffffffffffffff0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000ffffffffffffffffffffffff0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000100010000000000}}