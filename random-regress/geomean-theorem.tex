\documentclass{article}
%\usepackage{url}
\input{preamble-stuff}

\begin{document}

\section{Geometric Mean Result}

Let $\theta_{ijk}$, $j=1,2$ be the conditional probability of child nodes with value 1,2. Let $p_{ijk}$ be the frequency of parent-child instantiation in the database {\em for the target entity} whose value is specified by $k$. Then $p_{i1k} + p_{i2k} = 1$. E.g., the frequency of courses taken by Jack that are difficult plus frequency that are easy add up to 1. Without the prior subtraction, the geometric mean gives the (unnormalized) log-linear prediction formula $ln(\theta_{i1k}) p_{i1k} + ln(\theta_{i2k}) p_{i2k}$. Let $p_{k}$ be the prior of child node having value $k$. Setting now $w_{ijk} = ln(\theta_{ijk}) - ln(p_{k})$, plus adding a unit node with weight $w_{i0k} = ln(p_{k})$ we have the classification formula $w_{i1k} p_{i1k} + w_{i2k} p_{i2k} + w_{i0k} p_{i0k}$. This is equal to $(ln(\theta_{i1k}) - ln(p_{k}) p_{i1k} + (ln(\theta_{i2k} - ln(p_{k}) p_{i2k} + w_{i0k} p_{i0k}$. This equals $ln(\theta_{i1k}) p_{i1k} - ln(p_{k}) p_{i1k} + ln(\theta_{i2k}) p_{i2k} - ln(p_{k}) p_{i2k} + ln(p_{k}) p_{i0k}.$ Now $p_{i0k} = 1$ since all the variables are ground. Rearraning terms, we have $ ln(p_{k}) - ln(p_{k}) (p_{i1k}+p_{i2k})   + ln(\theta_{i1k}) p_{i1k}+ ln(\theta_{i2k}) p_{i2k}.$ Since $(p_{i1k}+p_{i2k}) = 1$, this simplifies to $ln(\theta_{i1k}) p_{i1k}+ ln(\theta_{i2k}) p_{i2k}$ as claimed.

\end{document}