{\rtf1\ansi\ansicpg1252\cocoartf1138\cocoasubrtf510
{\fonttbl\f0\fnil\fcharset0 Calibri;}
{\colortbl;\red255\green255\blue255;}
\vieww12000\viewh15840\viewkind0
\deftab720
\pard\pardeftab720\sl276\slmult1\sa200

\f0\fs22 \cf0 Notes on MLJ revision\
1. Background. See Russell and Norvig, Ullmann, Poole, Chiang, BLOG. GET THESE BOOKS.\
1.1. Logic\
1.1.1. Syntax\
1.1.2. Semantics. Quote Poole.\
1.1.3. Database Queries. The Cardinality Estimation Problem.\
1.1.4. First-order Logic with Probabilities\
1.1.4.5. Syntax\
1.1.4.6. Semantics. Use possible worlds.\
1.1.4.6.1. Probability Logic. Type 1 Semantics.\
1.1.4.6.2. Type 2 Semantics. Uses events. degrees of belief vs. statistics.\
A semantics that specifies truth conditions for probability sentences is a probabilistic semantics for 1st-order sentences.\
Can translate Type 2 into Type 1 Semantics.\
1.2. Bayes Nets\
1.2.1. Syntax\
1.2.2. Semantics quote Russell and Norvig. Unique Names Assumption.\
1.3. Bayes Nets For Relational Structures\
1.3.1. Syntax: functor syntax. PRV, Functor RV. Simple functors. Point to BLOG, aggregate functions.\
1.3.2. Type 2 Semantics: Knowledge-based Model Construction.\
Type 1 Semantics is missing. This is what we supply in this paper.\
Section 2 Random Selection Semantics for Bayes Nets\
2.1. Probabilistic Semantics. Functors Nodes as Random Variables.\
2.1. Truth-Conditional Semantics. Bayes Nets -> P1 sentences = probabilistic knowledge base.\
Show commuting diagram for joint probabilities over conjunctions of literals.\
Section 3. Learning.\
Database = data sample. possible world = complete population.\
3.1. Random Selection Likelihood. Give example.\
3.2. Consistency result. Interpretation: depends on sampling mechanism. Subgraph sampling has ergodicity result. Example: crawl web pages, note links among crawled ones. See other sampling mechanisms. Not active learning.\
Section 4. Computing Estimates.\
4.1. All true relationships.\
4.2. Single False Relationship. Solved by Getoor et al. Give example. \
Discussion: closed world vs. open world vs. missing data. Subsampling vs. Em. Database semantics.\
Lifted inference vs. lifted learning.\
4.3. Mobius transform: Extends to multiple relationships. \
5. Evaluation: add MLNs.\
\
\
}